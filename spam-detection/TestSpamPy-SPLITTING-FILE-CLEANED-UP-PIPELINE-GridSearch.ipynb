{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import precision_score, classification_report, accuracy_score\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    file_name = './SpamDetectionData.txt'\n",
    "    rawdata = open(file_name, 'r')\n",
    "    lines = rawdata.readlines()\n",
    "    lines = lines[1:] #get rid of \"header\"\n",
    "    spam_train = lines[0:1000]\n",
    "    ham_train = lines[1002:2002]\n",
    "    test_mix = lines[2004:]\n",
    "    return (spam_train, ham_train, test_mix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_train, ham_train, test_mix = get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = int(len(ham_train) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = spam_train + ham_train[0:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataframe(input_array):    \n",
    "    spam_indcator = 'Spam,<p>'\n",
    "    message_class = np.array([1 if spam_indcator in item else 0 for item in input_array])\n",
    "    data = pd.DataFrame()\n",
    "    data['class'] = message_class\n",
    "    data['message'] = input_array\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = create_dataframe(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = create_dataframe(test_mix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_to_remove = ['Ham,<p>', 'Spam,<p>', '<p>', '</p>', '\\n']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_words(input_line, key_words=words_to_remove):\n",
    "    temp = input_line\n",
    "    for word in key_words:\n",
    "        temp = temp.replace(word, '')\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_words_and_shuffle(input_dataframe):\n",
    "    input_dataframe['message'] = input_dataframe['message'].apply(remove_words)\n",
    "    messages, classes = shuffle(input_dataframe['message'], input_dataframe['class'], random_state=7)\n",
    "    df_return = pd.DataFrame()\n",
    "    df_return['class'] = classes\n",
    "    df_return['message'] = messages\n",
    "    return df_return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_cleaned = remove_words_and_shuffle(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_cleaned = remove_words_and_shuffle(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_raw = df_train_cleaned['message']\n",
    "y_train = df_train_cleaned['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_raw = df_test_cleaned['message']\n",
    "y_test = df_test_cleaned['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_accuracy(actual_y, predicted_y, model_name):\n",
    "    print(\"Model Name: \" + model_name)\n",
    "    print(\"Test accuracy: {:.4f}\".format(accuracy_score(actual_y, predicted_y)))\n",
    "    print(\"Test precision: {:.4f}\".format(precision_score(actual_y, predicted_y)))\n",
    "    print(\"\")\n",
    "    print(classification_report(actual_y, predicted_y, digits=4))\n",
    "    print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_models(X_train_input_raw, y_train_input, X_test_input_raw, y_test_input, models_dict):\n",
    "\n",
    "    return_trained_models = {}\n",
    "    \n",
    "    #return_vectorizer = TfidfVectorizer()\n",
    "    return_vectorizer = FeatureUnion([('tfidf_vect', TfidfVectorizer()), ('count_vect', CountVectorizer(min_df=.05,max_df=.8))])\n",
    "    \n",
    "    X_train = return_vectorizer.fit_transform(X_train_input_raw)\n",
    "    X_test = return_vectorizer.transform(X_test_input_raw)\n",
    "    \n",
    "    for key in models_dict:\n",
    "        model_name = key\n",
    "        model = models_dict[key]\n",
    "        model.fit(X_train, y_train_input)\n",
    "        predicted_y = model.predict(X_test)\n",
    "        \n",
    "        output_accuracy(y_test_input, predicted_y, model_name)\n",
    "        \n",
    "        return_trained_models[model_name] = model\n",
    "        \n",
    "    return (return_trained_models, return_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_models():\n",
    "    models = {}\n",
    "    models['LinearSVC'] = LinearSVC()\n",
    "    models['LogisticRegression'] = LogisticRegression()\n",
    "    models['RandomForestClassifier'] = RandomForestClassifier()\n",
    "    models['DecisionTreeClassifier'] = DecisionTreeClassifier()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: LinearSVC\n",
      "Test accuracy: 1.0000\n",
      "Test precision: 1.0000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    1.0000    1.0000        57\n",
      "          1     1.0000    1.0000    1.0000        43\n",
      "\n",
      "avg / total     1.0000    1.0000    1.0000       100\n",
      "\n",
      "=========================================================================\n",
      "Model Name: RandomForestClassifier\n",
      "Test accuracy: 0.9600\n",
      "Test precision: 0.9149\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    0.9298    0.9636        57\n",
      "          1     0.9149    1.0000    0.9556        43\n",
      "\n",
      "avg / total     0.9634    0.9600    0.9602       100\n",
      "\n",
      "=========================================================================\n",
      "Model Name: LogisticRegression\n",
      "Test accuracy: 0.9900\n",
      "Test precision: 0.9773\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    0.9825    0.9912        57\n",
      "          1     0.9773    1.0000    0.9885        43\n",
      "\n",
      "avg / total     0.9902    0.9900    0.9900       100\n",
      "\n",
      "=========================================================================\n",
      "Model Name: DecisionTreeClassifier\n",
      "Test accuracy: 0.9300\n",
      "Test precision: 0.8600\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     1.0000    0.8772    0.9346        57\n",
      "          1     0.8600    1.0000    0.9247        43\n",
      "\n",
      "avg / total     0.9398    0.9300    0.9303       100\n",
      "\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "models = create_models()\n",
    "trained_models, fitted_vectorizer = test_models(X_train_raw, y_train, X_test_raw, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row = df_test[1:2]\n",
    "test_msg = row['message']\n",
    "test_class = row['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    Again on quaff nothing. It explore stood usby ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_msg = ['this is my test message']\n",
    "transformed_test_msg = fitted_vectorizer.transform(test_msg)\n",
    "prediction = trained_models['DecisionTreeClassifier'].predict(transformed_test_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import out grid search module\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def get_best_model_and_accuracy(model, params, X, y):\n",
    "    grid = GridSearchCV(model, # the model to grid search\n",
    "                        params, # the parameter set to try \n",
    "                        error_score=0.) # if a parameter set raises an error, continue and set the performance as a big, fat 0\n",
    "    grid.fit(X, y) # fit the model and parameters\n",
    "    # our classical metric for performance\n",
    "    print(\"Best Accuracy: {}\".format(grid.best_score_))\n",
    "    # the best parameters that caused the best accuracy\n",
    "    print(\"Best Parameters: {}\".format(grid.best_params_))\n",
    "    # the average time it took a model to fit to the data (in seconds)\n",
    "    print(\"Average Time to Fit (s): {}\".format(round(grid.cv_results_['mean_fit_time'].mean(), 3)))\n",
    "    # the average time it took a model to predict out of sample data (in seconds)\n",
    "    # this metric gives us insight into how this model will perform in real-time analysis\n",
    "    print(\"Average Time to Score (s): {}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
